{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIZ84Nsjka0I"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEua-2NQob9N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYr09LZGoflk"
      },
      "outputs": [],
      "source": [
        "def load_data(path, split=0.1):\n",
        "  images = sorted(glob(os.path.join(path, \"images/*\" )))\n",
        "  masks = sorted(glob(os.path.join(path, \"masks/*\" )))\n",
        "\n",
        "  total_size = len(images)\n",
        "  valid_size = int(split * total_size)\n",
        "  test_size = int(split * total_size)\n",
        "\n",
        "  #print(total_size, valid_size, test_size)\n",
        "\n",
        "  train_x, valid_x = train_test_split(images, test_size = valid_size, random_state = 42)\n",
        "  train_y, valid_y = train_test_split(masks, test_size = valid_size, random_state = 42)\n",
        "\n",
        "  train_x, test_x = train_test_split(train_x, test_size = test_size, random_state = 42)\n",
        "  train_y, test_y = train_test_split(train_y, test_size = test_size, random_state = 42)\n",
        "\n",
        "  return(train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
        "\n",
        "def read_image(path):\n",
        "  path = path.decode()\n",
        "  x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "  x = cv2.resize(x, (256,256))\n",
        "  x = x/255.0    \n",
        "  return x\n",
        "\n",
        "def read_mask(path):\n",
        "  path = path.decode()\n",
        "  x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "  x = cv2.resize(x, (256,256))\n",
        "  x = x/255.0\n",
        "  x = np.expand_dims(x, axis = -1)    \n",
        "  return x\n",
        "\n",
        "def tf_parse(x, y):\n",
        "  def _parse(x, y):\n",
        "    x = read_image(x)\n",
        "    y = read_mask(y)\n",
        "    return x, y\n",
        "\n",
        "  x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.float64])\n",
        "  x.set_shape([256, 256, 3])\n",
        "  y.set_shape([256, 256, 1])\n",
        "\n",
        "  return x, y\n",
        "\n",
        "def tf_dataset(x, y, batch=8):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "  dataset = dataset.map(tf_parse)\n",
        "  dataset = dataset.batch(batch)\n",
        "  dataset = dataset.repeat()\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy6fPFTgonq7"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/Polyp Work/Dataset\"\n",
        "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(path)\n",
        "print(len(train_x), len(valid_x), len(test_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbE3KPLJvMbH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose\n",
        "\n",
        "def resunet():\n",
        "    input_shape=(256,256,3)\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    conv1_1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1_2 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1_1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_2)\n",
        "\n",
        "    conv2_1 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2_2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2_1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2_2)\n",
        "\n",
        "    conv3_1 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3_2 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3_1)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3_2)\n",
        "\n",
        "    conv4_1 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4_2 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4_1)\n",
        "\n",
        "    # Decoder\n",
        "    up5 = concatenate([Conv2DTranspose(256, 2, strides=(2,2), padding='same')(conv4_2), conv3_2], axis=-1)\n",
        "    conv5_1 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up5)\n",
        "    conv5_2 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5_1)\n",
        "\n",
        "    up6 = concatenate([Conv2DTranspose(128, 2, strides=(2,2), padding='same')(conv5_2), conv2_2], axis=-1)\n",
        "    conv6_1 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up6)\n",
        "    conv6_2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6_1)\n",
        "\n",
        "    up7 = concatenate([Conv2DTranspose(64, 2, strides=(2,2), padding='same')(conv6_2), conv1_2], axis=-1)\n",
        "    conv7_1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up7)\n",
        "    conv7_2 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer= 'he_normal')(conv7_1)\n",
        "\n",
        "\n",
        "    output = Conv2D(1, 1, activation='sigmoid')(conv7_2)\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRYP4392vj0s"
      },
      "outputs": [],
      "source": [
        "model = resunet()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2jx3yjxwGS1"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
        "from tensorflow.keras.metrics import Recall, Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M8EoJgJwkeC"
      },
      "outputs": [],
      "source": [
        "def iou(y_true, y_pred):\n",
        "  def f(y_true, y_pred):\n",
        "    intersection = (y_true * y_pred).sum()\n",
        "    union = y_true.sum() + y_pred.sum() - intersection\n",
        "    x = (intersection + 1e-15) / (union + 1e-15)\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "  return tf.numpy_function(f, [y_true, y_pred], tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qfai41PKwnKR"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "batch = 8\n",
        "lr = 1e-4\n",
        "epochs = 100\n",
        "\n",
        "train_dataset = tf_dataset(train_x, train_y, batch = batch)\n",
        "valid_dataset = tf_dataset(valid_x, valid_y, batch = batch)\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr)\n",
        "metrics = [\"acc\", Recall(), Precision(), iou]\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer = opt, metrics = metrics)\n",
        "\n",
        "callbacks = [ ModelCheckpoint(\"/content/drive/MyDrive/Polyp Work/ResUNet.hdf5\"),\n",
        "    ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 3),\n",
        "    TensorBoard(),\n",
        "    EarlyStopping(monitor = \"val_loss\", patience = 10, restore_best_weights = False)\n",
        "]\n",
        "\n",
        "\n",
        "train_steps = len(train_x) // batch\n",
        "valid_steps = len(valid_x) // batch\n",
        "\n",
        "if len(train_x) % batch !=0:\n",
        "    train_steps += 1\n",
        "\n",
        "if len(valid_x) % batch !=0:\n",
        "    valid_steps += 1\n",
        "  \n",
        "history = model.fit(\n",
        "    train_dataset, \n",
        "    validation_data = valid_dataset,\n",
        "    epochs = epochs,\n",
        "    steps_per_epoch = train_steps,\n",
        "    validation_steps = valid_steps,\n",
        "    callbacks = callbacks,\n",
        "    shuffle = False\n",
        "  )\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ0fk7Xd3S2o"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/drive/MyDrive/Polyp Work/ResUNet.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pkkY5MT4gSy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKiG19eF4lbX",
        "outputId": "6c494db9-47d6-4153-87f8-ee219a1cfa11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9856224656105042\n",
            "Training loss: 0.02782519720494747\n"
          ]
        }
      ],
      "source": [
        "acc = history.history['acc'][-1]\n",
        "loss = history.history['loss'][-1]\n",
        "print('Training Accuracy:',acc);\n",
        "print('Training loss:',loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_-UShvH4pfn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "\n",
        "test_dataset = tf_dataset(test_x, test_y, batch = batch)\n",
        "test_steps = len(test_x)//batch\n",
        "if len(test_x) % batch != 0:\n",
        "    test_steps += 1\n",
        "\n",
        "with CustomObjectScope({'iou': iou}):\n",
        "  model = tf.keras.models.load_model(\"/content/drive/MyDrive/Polyp Work/ResUNet.hdf5\")\n",
        "\n",
        "model.evaluate(test_dataset, steps = test_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nYIte1C4uYm"
      },
      "outputs": [],
      "source": [
        "def read_image(path):\n",
        "  x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "  y = cv2.cvtColor(x, cv2.COLOR_RGB2BGR)\n",
        "  y = cv2.resize(y, (256,256))\n",
        "  y = y/255.0\n",
        "  x = cv2.resize(x, (256,256))\n",
        "  x = x/255.0    #(256, 256, 3)\n",
        "  return x, y\n",
        "\n",
        "def read_mask(path):\n",
        "  x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "  x = cv2.resize(x, (256,256))\n",
        "  x = np.expand_dims(x, axis = -1)    #(256, 256, 1)\n",
        "  return x\n",
        "\n",
        "def mask_parse(mask):\n",
        "    mask = np.squeeze(mask)\n",
        "    mask = [mask, mask, mask]\n",
        "    mask = np.transpose(mask, (1, 2, 0))\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Q6mO6C45p6"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbjnz-gT4-2W"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total = len(test_x)):\n",
        "        x, rgb_img = read_image(x)\n",
        "        y = read_mask(y)\n",
        "                \n",
        "        y_pred = model.predict(np.expand_dims(x, axis = 0))\n",
        "        y_pred = y_pred[0] > 0.5\n",
        "        h, w, _ = x.shape\n",
        "        \n",
        "        #x = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        white_line = np.ones((h, 10, 3)) * 255.0\n",
        "        \n",
        "        all_images = [\n",
        "            rgb_img, white_line, \n",
        "            mask_parse(y), white_line,\n",
        "            mask_parse(y_pred) * 255.0\n",
        "        ]\n",
        "        \n",
        "        image = np.concatenate(all_images, axis = 1)\n",
        "        \n",
        "        plt.imshow(image)\n",
        "        plt.show()\n",
        "        cv2.imwrite(f\"/content/drive/MyDrive/result/{i}.png\", image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRfoLiIO6imA",
        "outputId": "4b6acbc7-4ecd-47e9-acc8-d16c028437be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keract\n",
            "  Downloading keract-4.5.1-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: keract\n",
            "Successfully installed keract-4.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install keract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyWyTkJz6nZX"
      },
      "outputs": [],
      "source": [
        "img_path = '/content/drive/MyDrive/Polyp Work/Dataset/images/1.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCRjTTyr6svo"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def preprocess_image(img_path, target_size):\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize(target_size)\n",
        "    x = np.array(img)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoE84bRD6vv8"
      },
      "outputs": [],
      "source": [
        "from keract import get_activations\n",
        "x = np.expand_dims(x, axis=0)\n",
        "#x = np.squeeze(x, axis=0)\n",
        "activations = get_activations(model, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9dDym0S6zH3"
      },
      "outputs": [],
      "source": [
        "from keract import display_activations\n",
        "display_activations(activations, save=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}